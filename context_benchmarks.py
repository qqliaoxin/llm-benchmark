import asyncio
import time
import json
import argparse
import random
import logging
from openai import AsyncOpenAI
import numpy as np
from rich.console import Console
from rich.table import Table
from rich.panel import Panel
from rich.text import Text

# å°è¯•å¯¼å…¥tiktokenè¿›è¡Œæ›´ç²¾ç¡®çš„tokenè®¡ç®—
try:
    import tiktoken
    TIKTOKEN_AVAILABLE = True
except ImportError:
    TIKTOKEN_AVAILABLE = False
    logging.info("tiktoken not available, using character-based estimation")

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def calculate_tokens_accurate(text, model_name="gpt-3.5-turbo"):
    """æ›´ç²¾ç¡®çš„tokenè®¡ç®—å‡½æ•°"""
    if not text:
        return 0
        
    if TIKTOKEN_AVAILABLE:
        try:
            # å°è¯•ä½¿ç”¨tiktokenè¿›è¡Œç²¾ç¡®è®¡ç®—
            if "qwen" in model_name.lower():
                # Qwenæ¨¡å‹ä½¿ç”¨cl100k_baseç¼–ç 
                encoding = tiktoken.get_encoding("cl100k_base")
            elif "deepseek" in model_name.lower():
                # DeepSeekæ¨¡å‹ä¹Ÿä½¿ç”¨cl100k_baseç¼–ç 
                encoding = tiktoken.get_encoding("cl100k_base")
            else:
                # é»˜è®¤ä½¿ç”¨cl100k_baseç¼–ç 
                encoding = tiktoken.get_encoding("cl100k_base")
            
            tokens = encoding.encode(text)
            return len(tokens)
        except Exception as e:
            logging.debug(f"tiktoken encoding failed: {e}, falling back to estimation")
    
    # å›é€€åˆ°æ”¹è¿›çš„å­—ç¬¦ä¼°ç®—æ–¹æ³•
    chinese_chars = len([c for c in text if '\u4e00' <= c <= '\u9fff'])
    english_chars = len(text) - chinese_chars
    
    # æ›´ç²¾ç¡®çš„tokenä¼°ç®—
    chinese_tokens = chinese_chars / 1.5 if chinese_chars > 0 else 0
    english_tokens = english_chars / 4.0 if english_chars > 0 else 0
    
    return max(1, int(chinese_tokens + english_tokens))

# ä¸åŒå¤§å°çš„ä¸Šä¸‹æ–‡æ¨¡æ¿
CONTEXT_TEMPLATES = {
    "13t": {
        "size": "13t",
        "context": "è¯·é‡å¤è¿™å¥è¯ï¼šè¿™æ˜¯ä¸€ä¸ªæµ‹è¯•"
    },
    "1k": {
        "size": "1k",
        "context": "äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè‡´åŠ›äºåˆ›å»ºèƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„æ™ºèƒ½æœºå™¨ã€‚è¿™äº›ä»»åŠ¡åŒ…æ‹¬å­¦ä¹ ã€æ¨ç†ã€é—®é¢˜è§£å†³ã€æ„ŸçŸ¥å’Œè¯­è¨€ç†è§£ã€‚AIç³»ç»Ÿå¯ä»¥åˆ†ä¸ºä¸¤å¤§ç±»ï¼šçª„AIï¼ˆä¸“é—¨è®¾è®¡ç”¨äºç‰¹å®šä»»åŠ¡ï¼‰å’Œé€šç”¨AIï¼ˆå…·æœ‰ç±»ä¼¼äººç±»çš„è®¤çŸ¥èƒ½åŠ›ï¼‰ã€‚æœºå™¨å­¦ä¹ æ˜¯AIçš„ä¸€ä¸ªå­é›†ï¼Œä½¿è®¡ç®—æœºèƒ½å¤Ÿä»æ•°æ®ä¸­å­¦ä¹ è€Œæ— éœ€æ˜ç¡®ç¼–ç¨‹ã€‚æ·±åº¦å­¦ä¹ è¿›ä¸€æ­¥åˆ©ç”¨ç¥ç»ç½‘ç»œæ¥æ¨¡æ‹Ÿäººè„‘çš„å·¥ä½œæ–¹å¼ã€‚AIåœ¨å„ä¸ªé¢†åŸŸéƒ½æœ‰åº”ç”¨ï¼ŒåŒ…æ‹¬åŒ»ç–—ä¿å¥ã€é‡‘èã€äº¤é€šå’Œå¨±ä¹ã€‚éšç€æŠ€æœ¯çš„è¿›æ­¥ï¼ŒAIæ­£åœ¨æ”¹å˜æˆ‘ä»¬çš„å·¥ä½œå’Œç”Ÿæ´»æ–¹å¼ï¼Œä½†ä¹Ÿå¸¦æ¥äº†å…³äºå°±ä¸šã€éšç§å’Œä¼¦ç†çš„é‡è¦é—®é¢˜ã€‚" * 2
    },
    "2k": {
        "size": "2k", 
        "context": "æ°”å€™å˜åŒ–æ˜¯æŒ‡åœ°çƒæ°”å€™ç³»ç»Ÿé•¿æœŸçš„å˜åŒ–ï¼Œä¸»è¦ç”±äººç±»æ´»åŠ¨å¼•èµ·çš„æ¸©å®¤æ°”ä½“æ’æ”¾æ‰€é©±åŠ¨ã€‚è‡ªå·¥ä¸šé©å‘½ä»¥æ¥ï¼Œå¤§æ°”ä¸­äºŒæ°§åŒ–ç¢³çš„æµ“åº¦æ€¥å‰§å¢åŠ ï¼Œä¸»è¦æ¥æºäºåŒ–çŸ³ç‡ƒæ–™çš„ç‡ƒçƒ§ã€æ£®æ—ç ä¼å’Œå·¥ä¸šè¿‡ç¨‹ã€‚è¿™å¯¼è‡´äº†å…¨çƒå¹³å‡æ¸©åº¦çš„ä¸Šå‡ï¼Œè¢«ç§°ä¸ºå…¨çƒå˜æš–ã€‚æ°”å€™å˜åŒ–çš„å½±å“æ˜¯å¤šæ–¹é¢çš„ï¼šæµ·å¹³é¢ä¸Šå‡å¨èƒæ²¿æµ·åœ°åŒºï¼Œæç«¯å¤©æ°”äº‹ä»¶å˜å¾—æ›´åŠ é¢‘ç¹å’Œä¸¥é‡ï¼Œç”Ÿæ€ç³»ç»Ÿå—åˆ°ç ´åï¼Œå†œä¸šç”Ÿäº§é¢ä¸´æŒ‘æˆ˜ã€‚åŒ—æå†°ç›–èåŒ–ï¼Œæ°¸ä¹…å†»åœŸå±‚è§£å†»ï¼Œè¿™äº›éƒ½è¿›ä¸€æ­¥åŠ å‰§äº†æ°”å€™å˜åŒ–ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œå›½é™…ç¤¾ä¼šåˆ¶å®šäº†ã€Šå·´é»åå®šã€‹ç­‰å›½é™…åè®®ï¼Œæ—¨åœ¨é™åˆ¶å…¨çƒæ¸©å‡ã€‚å‡ç¼“æ°”å€™å˜åŒ–éœ€è¦è½¬å‘å¯å†ç”Ÿèƒ½æºï¼Œæé«˜èƒ½æºæ•ˆç‡ï¼Œå‘å±•ç¢³æ•è·æŠ€æœ¯ï¼Œä»¥åŠæ”¹å˜ç”Ÿæ´»æ–¹å¼ã€‚é€‚åº”æ°”å€™å˜åŒ–åŒæ ·é‡è¦ï¼ŒåŒ…æ‹¬å»ºè®¾æŠ—ç¾åŸºç¡€è®¾æ–½ï¼Œå¼€å‘æŠ—æ—±ä½œç‰©ï¼Œä»¥åŠåˆ¶å®šåº”æ€¥é¢„æ¡ˆã€‚æ¯ä¸ªäººéƒ½å¯ä»¥é€šè¿‡å‡å°‘ç¢³è¶³è¿¹æ¥ä¸ºåº”å¯¹æ°”å€™å˜åŒ–åšå‡ºè´¡çŒ®ã€‚" * 3
    },
    "4k": {
        "size": "4k",
        "context": "é‡å­è®¡ç®—æ˜¯ä¸€ç§åˆ©ç”¨é‡å­åŠ›å­¦ç°è±¡è¿›è¡Œä¿¡æ¯å¤„ç†çš„è®¡ç®—èŒƒå¼ã€‚ä¸ä¼ ç»Ÿè®¡ç®—æœºä½¿ç”¨æ¯”ç‰¹ï¼ˆ0æˆ–1ï¼‰ä¸åŒï¼Œé‡å­è®¡ç®—æœºä½¿ç”¨é‡å­æ¯”ç‰¹ï¼ˆqubitsï¼‰ï¼Œå®ƒä»¬å¯ä»¥åŒæ—¶å¤„äº0å’Œ1çš„å åŠ çŠ¶æ€ã€‚è¿™ç§ç‰¹æ€§ï¼ŒåŠ ä¸Šé‡å­çº ç¼ å’Œé‡å­å¹²æ¶‰ï¼Œä½¿é‡å­è®¡ç®—æœºèƒ½å¤Ÿå¹¶è¡Œå¤„ç†å¤§é‡ä¿¡æ¯ï¼Œåœ¨æŸäº›é—®é¢˜ä¸Šæä¾›æŒ‡æ•°çº§çš„é€Ÿåº¦æå‡ã€‚é‡å­è®¡ç®—çš„æ ¸å¿ƒæ¦‚å¿µåŒ…æ‹¬å åŠ æ€ã€çº ç¼ å’Œæµ‹é‡ã€‚å åŠ æ€å…è®¸é‡å­æ¯”ç‰¹åŒæ—¶å­˜åœ¨äºå¤šä¸ªçŠ¶æ€ï¼Œè€Œçº ç¼ åˆ™åˆ›å»ºäº†é‡å­æ¯”ç‰¹ä¹‹é—´çš„å¼ºç›¸å…³æ€§ï¼Œå³ä½¿å®ƒä»¬åœ¨ç‰©ç†ä¸Šåˆ†ç¦»ã€‚é‡å­ç®—æ³•å¦‚Shorç®—æ³•ï¼ˆç”¨äºå› æ•°åˆ†è§£ï¼‰å’ŒGroverç®—æ³•ï¼ˆç”¨äºæœç´¢ï¼‰å±•ç¤ºäº†é‡å­è®¡ç®—çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œé‡å­è®¡ç®—é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬é‡å­é€€ç›¸å¹²ã€é”™è¯¯ç‡é«˜å’Œéœ€è¦æä½æ¸©åº¦çš„æ“ä½œç¯å¢ƒã€‚ç›®å‰çš„é‡å­è®¡ç®—æœºä»å¤„äºå™ªå£°ä¸­ç­‰è§„æ¨¡é‡å­ï¼ˆNISQï¼‰æ—¶ä»£ï¼Œå®¹æ˜“å—åˆ°ç¯å¢ƒå¹²æ‰°ã€‚å°½ç®¡å¦‚æ­¤ï¼ŒIBMã€Googleã€Microsoftç­‰å…¬å¸æ­£åœ¨ç§¯æå¼€å‘é‡å­è®¡ç®—æŠ€æœ¯ã€‚é‡å­è®¡ç®—çš„æ½œåœ¨åº”ç”¨åŒ…æ‹¬å¯†ç å­¦ã€è¯ç‰©å‘ç°ã€é‡‘èå»ºæ¨¡ã€äººå·¥æ™ºèƒ½å’Œææ–™ç§‘å­¦ã€‚é‡å­å¯†ç å­¦å·²ç»åœ¨æŸäº›é¢†åŸŸå¾—åˆ°åº”ç”¨ï¼Œæä¾›äº†ç†è®ºä¸Šä¸å¯ç ´è§£çš„é€šä¿¡å®‰å…¨ã€‚éšç€æŠ€æœ¯çš„è¿›æ­¥ï¼Œé‡å­è®¡ç®—æœ‰æœ›åœ¨æœªæ¥å‡ åå¹´å†…è§£å†³ä¸€äº›æœ€å¤æ‚çš„è®¡ç®—é—®é¢˜ï¼Œä½†ä¹Ÿå¯èƒ½å¯¹ç°æœ‰çš„åŠ å¯†æ–¹æ³•æ„æˆå¨èƒã€‚" * 4
    },
    "8k": {
        "size": "8k",
        "context": "ç”Ÿç‰©æŠ€æœ¯æ˜¯ä¸€ä¸ªè·¨å­¦ç§‘é¢†åŸŸï¼Œç»“åˆäº†ç”Ÿç‰©å­¦ã€åŒ–å­¦ã€ç‰©ç†å­¦ã€å·¥ç¨‹å­¦å’Œè®¡ç®—æœºç§‘å­¦ï¼Œåˆ©ç”¨ç”Ÿç‰©ç³»ç»Ÿã€ç”Ÿç‰©ä½“æˆ–å…¶è¡ç”Ÿç‰©æ¥å¼€å‘æˆ–åˆ¶é€ äº§å“ã€‚ç°ä»£ç”Ÿç‰©æŠ€æœ¯çš„å‘å±•å¯ä»¥è¿½æº¯åˆ°DNAé‡ç»„æŠ€æœ¯çš„å‘æ˜ï¼Œè¿™ä½¿å¾—ç§‘å­¦å®¶èƒ½å¤Ÿæ“çºµåŸºå› å¹¶åˆ›é€ è½¬åŸºå› ç”Ÿç‰©ã€‚åŸºå› å·¥ç¨‹æŠ€æœ¯åŒ…æ‹¬PCRï¼ˆèšåˆé…¶é“¾ååº”ï¼‰ã€åŸºå› å…‹éš†ã€åŸºå› æµ‹åºå’ŒCRISPR-Cas9åŸºå› ç¼–è¾‘ç³»ç»Ÿã€‚CRISPRæŠ€æœ¯ç‰¹åˆ«é©å‘½æ€§ï¼Œå› ä¸ºå®ƒå…è®¸ç²¾ç¡®ã€é«˜æ•ˆä¸”ç›¸å¯¹ä¾¿å®œçš„åŸºå› ç¼–è¾‘ã€‚ç”Ÿç‰©æŠ€æœ¯åœ¨åŒ»å­¦é¢†åŸŸæœ‰å¹¿æ³›åº”ç”¨ï¼ŒåŒ…æ‹¬é‡ç»„è›‹ç™½è¯ç‰©çš„ç”Ÿäº§ã€åŸºå› æ²»ç–—ã€ç»†èƒæ²»ç–—å’Œä¸ªæ€§åŒ–åŒ»å­¦ã€‚èƒ°å²›ç´ ã€ç”Ÿé•¿æ¿€ç´ å’Œå•å…‹éš†æŠ—ä½“ç­‰è¯ç‰©éƒ½æ˜¯é€šè¿‡ç”Ÿç‰©æŠ€æœ¯ç”Ÿäº§çš„ã€‚åœ¨å†œä¸šæ–¹é¢ï¼Œç”Ÿç‰©æŠ€æœ¯ç”¨äºå¼€å‘æŠ—è™«ã€æŠ—é™¤è‰å‰‚å’Œè¥å…»å¼ºåŒ–çš„è½¬åŸºå› ä½œç‰©ã€‚å·¥ä¸šç”Ÿç‰©æŠ€æœ¯åˆ©ç”¨å¾®ç”Ÿç‰©å’Œé…¶æ¥ç”Ÿäº§åŒ–å­¦å“ã€ç‡ƒæ–™å’Œææ–™ï¼Œæä¾›äº†æ›´ç¯ä¿çš„æ›¿ä»£æ–¹æ¡ˆã€‚åˆæˆç”Ÿç‰©å­¦æ˜¯ç”Ÿç‰©æŠ€æœ¯çš„æ–°å…´åˆ†æ”¯ï¼Œæ—¨åœ¨è®¾è®¡å’Œæ„å»ºæ–°çš„ç”Ÿç‰©éƒ¨ä»¶ã€è®¾å¤‡å’Œç³»ç»Ÿã€‚ç¯å¢ƒç”Ÿç‰©æŠ€æœ¯ç”¨äºæ±¡æŸ“æ²»ç†ã€åºŸç‰©å¤„ç†å’Œç¯å¢ƒç›‘æµ‹ã€‚ç”Ÿç‰©ä¿¡æ¯å­¦ç»“åˆäº†ç”Ÿç‰©å­¦å’Œè®¡ç®—æœºç§‘å­¦ï¼Œç”¨äºåˆ†æå’Œè§£é‡Šç”Ÿç‰©æ•°æ®ã€‚ç„¶è€Œï¼Œç”Ÿç‰©æŠ€æœ¯ä¹Ÿå¼•å‘äº†ä¼¦ç†ã€å®‰å…¨å’Œç›‘ç®¡æ–¹é¢çš„æ‹…å¿§ï¼ŒåŒ…æ‹¬è½¬åŸºå› é£Ÿå“çš„å®‰å…¨æ€§ã€åŸºå› ç¼–è¾‘çš„ä¼¦ç†ç•Œé™ä»¥åŠç”Ÿç‰©æ­¦å™¨çš„æ½œåœ¨é£é™©ã€‚éšç€æŠ€æœ¯çš„ä¸æ–­è¿›æ­¥ï¼Œç”Ÿç‰©æŠ€æœ¯æœ‰æœ›åœ¨è§£å†³å…¨çƒå¥åº·ã€é£Ÿå“å®‰å…¨å’Œç¯å¢ƒæŒ‘æˆ˜æ–¹é¢å‘æŒ¥è¶Šæ¥è¶Šé‡è¦çš„ä½œç”¨ã€‚" * 6
    },
    "16k": {
        "size": "16k",
        "context": "åŒºå—é“¾æŠ€æœ¯æ˜¯ä¸€ç§åˆ†å¸ƒå¼è´¦æœ¬æŠ€æœ¯ï¼Œé€šè¿‡å¯†ç å­¦æ–¹æ³•å°†æ•°æ®å—æŒ‰æ—¶é—´é¡ºåºé“¾æ¥èµ·æ¥ï¼Œå½¢æˆä¸€ä¸ªä¸å¯ç¯¡æ”¹çš„æ•°æ®é“¾ã€‚æ¯ä¸ªåŒºå—åŒ…å«å‰ä¸€ä¸ªåŒºå—çš„å“ˆå¸Œå€¼ã€æ—¶é—´æˆ³å’Œäº¤æ˜“æ•°æ®ï¼Œè¿™ç§è®¾è®¡ç¡®ä¿äº†æ•°æ®çš„å®Œæ•´æ€§å’Œä¸å¯ç¯¡æ”¹æ€§ã€‚åŒºå—é“¾çš„æ ¸å¿ƒç‰¹å¾åŒ…æ‹¬å»ä¸­å¿ƒåŒ–ã€é€æ˜æ€§ã€ä¸å¯ç¯¡æ”¹æ€§å’Œå…±è¯†æœºåˆ¶ã€‚å»ä¸­å¿ƒåŒ–æ„å‘³ç€æ²¡æœ‰å•ä¸€çš„æ§åˆ¶ç‚¹ï¼Œç½‘ç»œç”±å¤šä¸ªèŠ‚ç‚¹å…±åŒç»´æŠ¤ã€‚é€æ˜æ€§ç¡®ä¿æ‰€æœ‰äº¤æ˜“éƒ½æ˜¯å…¬å¼€å¯éªŒè¯çš„ï¼Œè€Œå…±è¯†æœºåˆ¶ï¼ˆå¦‚å·¥ä½œé‡è¯æ˜ã€æƒç›Šè¯æ˜ï¼‰ç¡®ä¿ç½‘ç»œå‚ä¸è€…å¯¹è´¦æœ¬çŠ¶æ€è¾¾æˆä¸€è‡´ã€‚æ¯”ç‰¹å¸æ˜¯ç¬¬ä¸€ä¸ªæˆåŠŸçš„åŒºå—é“¾åº”ç”¨ï¼Œå±•ç¤ºäº†æ•°å­—è´§å¸çš„å¯èƒ½æ€§ã€‚ä»¥å¤ªåŠè¿›ä¸€æ­¥æ‰©å±•äº†åŒºå—é“¾çš„åŠŸèƒ½ï¼Œå¼•å…¥äº†æ™ºèƒ½åˆçº¦æ¦‚å¿µï¼Œå…è®¸åœ¨åŒºå—é“¾ä¸Šæ‰§è¡Œè‡ªåŠ¨åŒ–çš„åˆçº¦é€»è¾‘ã€‚æ™ºèƒ½åˆçº¦æ˜¯è‡ªæ‰§è¡Œçš„åˆçº¦ï¼Œå…¶æ¡æ¬¾ç›´æ¥å†™å…¥ä»£ç ä¸­ï¼Œå½“é¢„å®šæ¡ä»¶æ»¡è¶³æ—¶è‡ªåŠ¨æ‰§è¡Œã€‚è¿™å¼€å¯äº†å»ä¸­å¿ƒåŒ–åº”ç”¨ï¼ˆDAppsï¼‰çš„æ—¶ä»£ï¼ŒåŒ…æ‹¬å»ä¸­å¿ƒåŒ–é‡‘èï¼ˆDeFiï¼‰ã€éåŒè´¨åŒ–ä»£å¸ï¼ˆNFTï¼‰å’Œå»ä¸­å¿ƒåŒ–è‡ªæ²»ç»„ç»‡ï¼ˆDAOï¼‰ã€‚DeFié‡æ–°æ„æƒ³äº†ä¼ ç»Ÿé‡‘èæœåŠ¡ï¼Œæä¾›å»ä¸­å¿ƒåŒ–çš„å€Ÿè´·ã€äº¤æ˜“å’Œä¿é™©æœåŠ¡ã€‚NFTä¸ºæ•°å­—è‰ºæœ¯å“å’Œæ”¶è—å“åˆ›é€ äº†æ–°çš„å¸‚åœºã€‚åŒºå—é“¾æŠ€æœ¯åœ¨ä¾›åº”é“¾ç®¡ç†ä¸­ä¹Ÿæœ‰é‡è¦åº”ç”¨ï¼Œæä¾›äº§å“ä»ç”Ÿäº§åˆ°æ¶ˆè´¹çš„å®Œæ•´å¯è¿½æº¯æ€§ã€‚åœ¨åŒ»ç–—ä¿å¥é¢†åŸŸï¼ŒåŒºå—é“¾å¯ä»¥å®‰å…¨åœ°å­˜å‚¨å’Œå…±äº«æ‚£è€…æ•°æ®ã€‚æ•°å­—èº«ä»½ç®¡ç†æ˜¯å¦ä¸€ä¸ªé‡è¦åº”ç”¨ï¼Œç”¨æˆ·å¯ä»¥æ§åˆ¶è‡ªå·±çš„èº«ä»½ä¿¡æ¯è€Œä¸ä¾èµ–ä¸­å¿ƒåŒ–æœºæ„ã€‚ç„¶è€Œï¼ŒåŒºå—é“¾æŠ€æœ¯ä¹Ÿé¢ä¸´æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬å¯æ‰©å±•æ€§é—®é¢˜ã€èƒ½æºæ¶ˆè€—ã€ç›‘ç®¡ä¸ç¡®å®šæ€§å’Œç”¨æˆ·ä½“éªŒé—®é¢˜ã€‚ç¬¬äºŒå±‚è§£å†³æ–¹æ¡ˆå¦‚é—ªç”µç½‘ç»œå’Œä¾§é“¾æ­£åœ¨è§£å†³å¯æ‰©å±•æ€§é—®é¢˜ã€‚æƒç›Šè¯æ˜ç­‰æ–°çš„å…±è¯†æœºåˆ¶æ­£åœ¨å‡å°‘èƒ½æºæ¶ˆè€—ã€‚éšç€æŠ€æœ¯çš„æˆç†Ÿå’Œç›‘ç®¡æ¡†æ¶çš„å®Œå–„ï¼ŒåŒºå—é“¾æœ‰æœ›åœ¨æ›´å¤šé¢†åŸŸå¾—åˆ°åº”ç”¨ï¼Œæ¨åŠ¨æ•°å­—ç»æµçš„å‘å±•ã€‚" * 8
    },
    "32k": {
        "size": "32k",
        "context": "ç¥ç»ç§‘å­¦æ˜¯ç ”ç©¶ç¥ç»ç³»ç»Ÿç»“æ„å’ŒåŠŸèƒ½çš„ç§‘å­¦é¢†åŸŸï¼Œæ¶µç›–äº†ä»åˆ†å­å’Œç»†èƒæ°´å¹³åˆ°è¡Œä¸ºå’Œè®¤çŸ¥æ°´å¹³çš„å„ä¸ªå±‚é¢ã€‚äººç±»å¤§è„‘åŒ…å«çº¦860äº¿ä¸ªç¥ç»å…ƒï¼Œé€šè¿‡æ•°ä¸‡äº¿ä¸ªçªè§¦è¿æ¥å½¢æˆå¤æ‚çš„ç¥ç»ç½‘ç»œã€‚ç¥ç»å…ƒæ˜¯ç¥ç»ç³»ç»Ÿçš„åŸºæœ¬å•ä½ï¼Œé€šè¿‡ç”µåŒ–å­¦ä¿¡å·è¿›è¡Œé€šä¿¡ã€‚ç¥ç»å…ƒçš„ç»“æ„åŒ…æ‹¬ç»†èƒä½“ã€æ ‘çªå’Œè½´çªï¼Œå…¶ä¸­æ ‘çªæ¥æ”¶ä¿¡å·ï¼Œè½´çªä¼ é€’ä¿¡å·ã€‚çªè§¦æ˜¯ç¥ç»å…ƒä¹‹é—´çš„è¿æ¥ç‚¹ï¼Œé€šè¿‡ç¥ç»é€’è´¨è¿›è¡ŒåŒ–å­¦ä¼ é€’ã€‚å¤§è„‘çš„ç»“æ„é«˜åº¦å¤æ‚ï¼ŒåŒ…æ‹¬å¤§è„‘çš®å±‚ã€å°è„‘ã€è„‘å¹²å’Œè¾¹ç¼˜ç³»ç»Ÿç­‰ä¸»è¦åŒºåŸŸã€‚å¤§è„‘çš®å±‚è´Ÿè´£é«˜çº§è®¤çŸ¥åŠŸèƒ½ï¼Œå¦‚æ€ç»´ã€è¯­è¨€å’Œæ„è¯†ã€‚å°è„‘ä¸»è¦è´Ÿè´£è¿åŠ¨åè°ƒå’Œå¹³è¡¡ã€‚è„‘å¹²æ§åˆ¶åŸºæœ¬ç”Ÿå‘½åŠŸèƒ½ï¼Œå¦‚å‘¼å¸å’Œå¿ƒè·³ã€‚è¾¹ç¼˜ç³»ç»Ÿå‚ä¸æƒ…ç»ªã€è®°å¿†å’ŒåŠ¨æœºã€‚ç¥ç»å¯å¡‘æ€§æ˜¯å¤§è„‘çš„ä¸€ä¸ªé‡è¦ç‰¹æ€§ï¼ŒæŒ‡ç¥ç»ç³»ç»Ÿæ ¹æ®ç»éªŒæ”¹å˜å…¶ç»“æ„å’ŒåŠŸèƒ½çš„èƒ½åŠ›ã€‚è¿™ç§å¯å¡‘æ€§æ˜¯å­¦ä¹ å’Œè®°å¿†çš„åŸºç¡€ï¼Œä¹Ÿæ˜¯å¤§è„‘æŸä¼¤ååº·å¤çš„æœºåˆ¶ã€‚è®°å¿†å½¢æˆæ¶‰åŠå¤šä¸ªå¤§è„‘åŒºåŸŸçš„åè°ƒå·¥ä½œï¼ŒåŒ…æ‹¬æµ·é©¬ä½“ã€æä»æ ¸å’Œæ–°çš®å±‚ã€‚é•¿æœŸè®°å¿†çš„å½¢æˆéœ€è¦è›‹ç™½è´¨åˆæˆå’Œçªè§¦å¼ºåº¦çš„æŒä¹…æ”¹å˜ã€‚ç¥ç»ç§‘å­¦ç ”ç©¶æ–¹æ³•åŒ…æ‹¬ç”µç”Ÿç†å­¦ã€ç¥ç»å½±åƒå­¦ã€å…‰é—ä¼ å­¦å’Œåˆ†å­ç”Ÿç‰©å­¦æŠ€æœ¯ã€‚åŠŸèƒ½æ€§ç£å…±æŒ¯æˆåƒï¼ˆfMRIï¼‰å’Œæ­£ç”µå­å‘å°„æ–­å±‚æ‰«æï¼ˆPETï¼‰ç­‰æŠ€æœ¯ä½¿æˆ‘ä»¬èƒ½å¤Ÿè§‚å¯Ÿæ´»ä½“å¤§è„‘çš„æ´»åŠ¨ã€‚å…‰é—ä¼ å­¦æŠ€æœ¯å…è®¸ç§‘å­¦å®¶ç”¨å…‰ç²¾ç¡®æ§åˆ¶ç‰¹å®šç¥ç»å…ƒçš„æ´»åŠ¨ã€‚ç¥ç»ç–¾ç—…å¦‚é˜¿å°”èŒ¨æµ·é»˜ç—…ã€å¸•é‡‘æ£®ç—…ã€æŠ‘éƒç—‡å’Œç²¾ç¥åˆ†è£‚ç—‡ä¸¥é‡å½±å“äººç±»å¥åº·ã€‚è¿™äº›ç–¾ç—…çš„ç ”ç©¶æ¨åŠ¨äº†æˆ‘ä»¬å¯¹å¤§è„‘åŠŸèƒ½çš„ç†è§£ï¼Œä¹Ÿä¿ƒè¿›äº†æ–°æ²»ç–—æ–¹æ³•çš„å¼€å‘ã€‚æ·±åº¦è„‘åˆºæ¿€ã€è¯ç‰©æ²»ç–—å’Œè®¤çŸ¥è¡Œä¸ºç–—æ³•ç­‰æ²»ç–—æ‰‹æ®µæ­£åœ¨ä¸æ–­æ”¹è¿›ã€‚è®¡ç®—ç¥ç»ç§‘å­¦ç»“åˆæ•°å­¦æ¨¡å‹å’Œè®¡ç®—æœºæ¨¡æ‹Ÿæ¥ç†è§£å¤§è„‘åŠŸèƒ½ï¼Œè¿™ä¸€é¢†åŸŸä¹Ÿæ¨åŠ¨äº†äººå·¥æ™ºèƒ½çš„å‘å±•ã€‚ç¥ç»ç½‘ç»œç®—æ³•å—åˆ°ç”Ÿç‰©ç¥ç»ç½‘ç»œçš„å¯å‘ï¼Œåœ¨æœºå™¨å­¦ä¹ ä¸­å–å¾—äº†å·¨å¤§æˆåŠŸã€‚è„‘æœºæ¥å£æŠ€æœ¯æ­£åœ¨å¼€å‘ä¸­ï¼Œæœ‰æœ›å¸®åŠ©ç˜«ç—ªæ‚£è€…æ¢å¤è¿åŠ¨èƒ½åŠ›ã€‚æ„è¯†ç ”ç©¶æ˜¯ç¥ç»ç§‘å­¦çš„å‰æ²¿é¢†åŸŸï¼Œè¯•å›¾ç†è§£ä¸»è§‚ä½“éªŒçš„ç¥ç»åŸºç¡€ã€‚è¿™æ¶‰åŠåˆ°å“²å­¦ã€å¿ƒç†å­¦å’Œç¥ç»ç§‘å­¦çš„äº¤å‰ã€‚éšç€æŠ€æœ¯çš„è¿›æ­¥ï¼Œæˆ‘ä»¬å¯¹å¤§è„‘çš„ç†è§£ä¸æ–­æ·±å…¥ï¼Œè¿™ä¸ä»…æœ‰åŠ©äºæ²»ç–—ç¥ç»ç–¾ç—…ï¼Œä¹Ÿå¯èƒ½æ­ç¤ºäººç±»è®¤çŸ¥å’Œæ„è¯†çš„å¥¥ç§˜ã€‚" * 12
    },
    "64k": {
        "size": "64k",
        "context": "å®‡å®™å­¦æ˜¯ç ”ç©¶å®‡å®™æ•´ä½“ç»“æ„ã€èµ·æºã€æ¼”åŒ–å’Œæœ€ç»ˆå‘½è¿çš„ç§‘å­¦ã€‚ç°ä»£å®‡å®™å­¦å»ºç«‹åœ¨çˆ±å› æ–¯å¦çš„å¹¿ä¹‰ç›¸å¯¹è®ºåŸºç¡€ä¸Šï¼Œç»“åˆäº†å¤©ä½“ç‰©ç†å­¦ã€ç²’å­ç‰©ç†å­¦å’Œè§‚æµ‹å¤©æ–‡å­¦çš„æœ€æ–°å‘ç°ã€‚å¤§çˆ†ç‚¸ç†è®ºæ˜¯ç›®å‰æœ€è¢«å¹¿æ³›æ¥å—çš„å®‡å®™èµ·æºæ¨¡å‹ï¼Œè®¤ä¸ºå®‡å®™å§‹äºçº¦138äº¿å¹´å‰çš„ä¸€ä¸ªæå…¶ç‚½çƒ­å’Œè‡´å¯†çš„çŠ¶æ€ï¼Œç„¶åç»å†äº†å¿«é€Ÿè†¨èƒ€å’Œå†·å´è¿‡ç¨‹ã€‚å®‡å®™å¾®æ³¢èƒŒæ™¯è¾å°„çš„å‘ç°ä¸ºå¤§çˆ†ç‚¸ç†è®ºæä¾›äº†å¼ºæœ‰åŠ›çš„è¯æ®ï¼Œè¿™æ˜¯å®‡å®™æ—©æœŸç•™ä¸‹çš„ä½™è¾‰ã€‚æš—ç‰©è´¨å’Œæš—èƒ½é‡æ˜¯ç°ä»£å®‡å®™å­¦ä¸­æœ€ç¥ç§˜çš„ç»„æˆéƒ¨åˆ†ï¼Œå®ƒä»¬åˆ†åˆ«å å®‡å®™æ€»è´¨é‡èƒ½é‡çš„çº¦27%å’Œ68%ï¼Œè€Œæˆ‘ä»¬ç†Ÿæ‚‰çš„æ™®é€šç‰©è´¨ä»…å çº¦5%ã€‚æš—ç‰©è´¨é€šè¿‡å¼•åŠ›æ•ˆåº”å½±å“æ˜Ÿç³»çš„å½¢æˆå’Œæ¼”åŒ–ï¼Œä½†ä¸ä¸ç”µç£è¾å°„ç›¸äº’ä½œç”¨ï¼Œå› æ­¤æ— æ³•ç›´æ¥è§‚æµ‹ã€‚æš—èƒ½é‡è¢«è®¤ä¸ºæ˜¯å¯¼è‡´å®‡å®™åŠ é€Ÿè†¨èƒ€çš„åŸå› ï¼Œä½†å…¶æœ¬è´¨ä»ç„¶æ˜¯ä¸ªè°œã€‚å®‡å®™çš„å¤§å°ºåº¦ç»“æ„å‘ˆç°å‡ºç½‘çŠ¶åˆ†å¸ƒï¼Œç”±æ˜Ÿç³»å›¢ã€æ˜Ÿç³»ç¾¤å’Œå·¨å¤§çš„ç©ºæ´ç»„æˆã€‚è¿™ç§ç»“æ„çš„å½¢æˆå¯ä»¥è¿½æº¯åˆ°å®‡å®™æ—©æœŸçš„å¾®å°å¯†åº¦æ¶¨è½ï¼Œè¿™äº›æ¶¨è½åœ¨å¼•åŠ›ä½œç”¨ä¸‹é€æ¸æ”¾å¤§ï¼Œæœ€ç»ˆå½¢æˆäº†æˆ‘ä»¬ä»Šå¤©è§‚å¯Ÿåˆ°çš„å®‡å®™ç»“æ„ã€‚æ’æ˜Ÿçš„ç”Ÿå‘½å‘¨æœŸå¯¹å®‡å®™çš„åŒ–å­¦æ¼”åŒ–èµ·ç€å…³é”®ä½œç”¨ï¼Œé‡å…ƒç´ åœ¨æ’æ˜Ÿå†…éƒ¨é€šè¿‡æ ¸èšå˜äº§ç”Ÿï¼Œå¹¶åœ¨è¶…æ–°æ˜Ÿçˆ†å‘æ—¶æ•£å¸ƒåˆ°å®‡å®™ä¸­ï¼Œä¸ºåç»­æ’æ˜Ÿå’Œè¡Œæ˜Ÿçš„å½¢æˆæä¾›äº†åŸæ–™ã€‚é»‘æ´æ˜¯å®‡å®™ä¸­æœ€æç«¯çš„å¤©ä½“ï¼Œå…·æœ‰å¦‚æ­¤å¼ºå¤§çš„å¼•åŠ›åœºï¼Œè¿å…‰éƒ½æ— æ³•é€ƒè„±ã€‚è¶…å¤§è´¨é‡é»‘æ´ä½äºå¤§å¤šæ•°æ˜Ÿç³»çš„ä¸­å¿ƒï¼Œå¯¹æ˜Ÿç³»çš„æ¼”åŒ–äº§ç”Ÿé‡è¦å½±å“ã€‚å¼•åŠ›æ³¢çš„æ¢æµ‹å¼€å¯äº†è§‚æµ‹å®‡å®™çš„æ–°çª—å£ï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿç ”ç©¶é»‘æ´åˆå¹¶ã€ä¸­å­æ˜Ÿç¢°æ’ç­‰æç«¯äº‹ä»¶ã€‚å®‡å®™å­¦å¸¸æ•°é—®é¢˜ã€å±‚æ¬¡é—®é¢˜å’Œæš—ç‰©è´¨çš„æœ¬è´¨ç­‰åŸºæœ¬é—®é¢˜ä»ç„¶å›°æ‰°ç€ç‰©ç†å­¦å®¶ã€‚å¤šå…ƒå®‡å®™ç†è®ºæå‡ºæˆ‘ä»¬çš„å®‡å®™å¯èƒ½åªæ˜¯æ— æ•°å®‡å®™ä¸­çš„ä¸€ä¸ªï¼Œä½†è¿™ä¸€ç†è®ºç›®å‰è¿˜æ— æ³•é€šè¿‡å®éªŒéªŒè¯ã€‚é‡å­å¼•åŠ›ç†è®ºè¯•å›¾ç»Ÿä¸€å¹¿ä¹‰ç›¸å¯¹è®ºå’Œé‡å­åŠ›å­¦ï¼Œå¯èƒ½ä¸ºç†è§£å®‡å®™çš„æœ€åˆæ—¶åˆ»æä¾›æ–°çš„æ´å¯Ÿã€‚å®‡å®™çš„æœ€ç»ˆå‘½è¿å–å†³äºæš—èƒ½é‡çš„æ€§è´¨ï¼Œå¯èƒ½çš„ç»“å±€åŒ…æ‹¬çƒ­å¯‚ã€å¤§æ’•è£‚æˆ–å¤§åç¼©ã€‚éšç€è§‚æµ‹æŠ€æœ¯çš„ä¸æ–­è¿›æ­¥ï¼Œå¦‚è©¹å§†æ–¯Â·éŸ¦ä¼¯å¤ªç©ºæœ›è¿œé•œç­‰æ–°ä¸€ä»£è®¾å¤‡ï¼Œæˆ‘ä»¬å¯¹å®‡å®™çš„ç†è§£å°†ç»§ç»­æ·±åŒ–ã€‚" * 16
    },
    "92k": {
        "size": "92k",
        "context": "è¿›åŒ–ç”Ÿç‰©å­¦æ˜¯ç ”ç©¶ç”Ÿç‰©å¤šæ ·æ€§èµ·æºå’Œå‘å±•çš„ç§‘å­¦é¢†åŸŸï¼Œæ¢ç´¢ç”Ÿå‘½å¦‚ä½•ä»ç®€å•çš„å½¢å¼æ¼”åŒ–ä¸ºä»Šå¤©æˆ‘ä»¬çœ‹åˆ°çš„å¤æ‚å¤šæ ·çš„ç”Ÿç‰©ä¸–ç•Œã€‚è¾¾å°”æ–‡çš„è‡ªç„¶é€‰æ‹©ç†è®ºä¸ºç°ä»£è¿›åŒ–ç”Ÿç‰©å­¦å¥ å®šäº†åŸºç¡€ï¼Œè¯¥ç†è®ºæå‡ºå…·æœ‰æœ‰åˆ©å˜å¼‚çš„ä¸ªä½“æ›´å¯èƒ½ç”Ÿå­˜å’Œç¹æ®–ï¼Œä»è€Œå°†è¿™äº›ç‰¹å¾ä¼ é€’ç»™åä»£ã€‚ç°ä»£ç»¼åˆç†è®ºå°†è¾¾å°”æ–‡çš„è‡ªç„¶é€‰æ‹©ä¸å­Ÿå¾·å°”é—ä¼ å­¦ã€åˆ†å­ç”Ÿç‰©å­¦å’Œç¾¤ä½“é—ä¼ å­¦ç›¸ç»“åˆï¼Œå½¢æˆäº†æ›´å®Œæ•´çš„è¿›åŒ–æ¡†æ¶ã€‚DNAå’ŒRNAçš„å‘ç°æ­ç¤ºäº†é—ä¼ ä¿¡æ¯çš„åˆ†å­åŸºç¡€ï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿåœ¨åŸºå› æ°´å¹³ä¸Šç†è§£è¿›åŒ–è¿‡ç¨‹ã€‚åˆ†å­é’ŸæŠ€æœ¯é€šè¿‡æ¯”è¾ƒä¸åŒç‰©ç§é—´çš„åŸºå› åºåˆ—å·®å¼‚æ¥ä¼°ç®—å®ƒä»¬çš„åˆ†åŒ–æ—¶é—´ï¼Œä¸ºæ„å»ºç”Ÿå‘½æ ‘æä¾›äº†é‡è¦å·¥å…·ã€‚åŒ–çŸ³è®°å½•è™½ç„¶ä¸å®Œæ•´ï¼Œä½†ä¸ºæˆ‘ä»¬æä¾›äº†ç”Ÿå‘½æ¼”åŒ–å†å²çš„ç›´æ¥è¯æ®ï¼Œå±•ç¤ºäº†ä»ç®€å•çš„å•ç»†èƒç”Ÿç‰©åˆ°å¤æ‚å¤šç»†èƒç”Ÿç‰©çš„æ¼”åŒ–è¿‡ç¨‹ã€‚å¯’æ­¦çºªå¤§çˆ†å‘æ˜¯ç”Ÿå‘½å²ä¸Šä¸€ä¸ªé‡è¦äº‹ä»¶ï¼Œåœ¨ç›¸å¯¹è¾ƒçŸ­çš„åœ°è´¨æ—¶é—´å†…å‡ºç°äº†å¤§é‡æ–°çš„åŠ¨ç‰©é—¨ç±»ã€‚å¤§ç­ç»äº‹ä»¶å¦‚äºŒå çºªæœ«å¤§ç­ç»å’Œç™½å©çºªæœ«å¤§ç­ç»æ·±åˆ»å½±å“äº†ç”Ÿå‘½çš„æ¼”åŒ–è½¨è¿¹ï¼Œä¸ºæ–°çš„ç”Ÿç‰©ç±»ç¾¤çš„è¾å°„æ¼”åŒ–åˆ›é€ äº†æœºä¼šã€‚é€‚åº”æ€§è¾å°„æ˜¯æŒ‡ä¸€ä¸ªç¥–å…ˆç‰©ç§åœ¨çŸ­æ—¶é—´å†…åˆ†åŒ–ä¸ºå¤šä¸ªé€‚åº”ä¸åŒç”Ÿæ€ä½çš„åä»£ç‰©ç§ï¼ŒåŠ æ‹‰å¸•æˆˆæ–¯é›€é¸Ÿæ˜¯ç»å…¸çš„ä¾‹å­ã€‚å…±åŒæ¼”åŒ–æè¿°äº†ä¸åŒç‰©ç§ä¹‹é—´ç›¸äº’å½±å“çš„æ¼”åŒ–è¿‡ç¨‹ï¼Œå¦‚æ¤ç‰©ä¸ä¼ ç²‰è€…ã€æ•é£Ÿè€…ä¸çŒç‰©ä¹‹é—´çš„ååŒæ¼”åŒ–ã€‚æ€§é€‰æ‹©æ˜¯è‡ªç„¶é€‰æ‹©çš„ä¸€ç§ç‰¹æ®Šå½¢å¼ï¼Œè§£é‡Šäº†è®¸å¤šçœ‹ä¼¼ä¸åˆ©äºç”Ÿå­˜çš„ç‰¹å¾çš„æ¼”åŒ–ï¼Œå¦‚å­”é›€çš„åä¸½å°¾ç¾½ã€‚åŸºå› æ¼‚å˜åœ¨å°ç¾¤ä½“ä¸­èµ·é‡è¦ä½œç”¨ï¼Œå¯èƒ½å¯¼è‡´æœ‰å®³åŸºå› çš„å›ºå®šæˆ–æœ‰åˆ©åŸºå› çš„ä¸¢å¤±ã€‚åˆ†å­è¿›åŒ–ç ”ç©¶æ­ç¤ºäº†åŸºå› å’Œè›‹ç™½è´¨çš„æ¼”åŒ–æ¨¡å¼ï¼Œå‘ç°äº†ä¸­æ€§æ¼”åŒ–å’Œæ­£é€‰æ‹©çš„è¯æ®ã€‚æ¯”è¾ƒåŸºå› ç»„å­¦é€šè¿‡æ¯”è¾ƒä¸åŒç‰©ç§çš„åŸºå› ç»„åºåˆ—ï¼Œæ­ç¤ºäº†åŸºå› åŠŸèƒ½çš„æ¼”åŒ–å’Œæ–°åŸºå› çš„èµ·æºã€‚è¡¨è§‚é—ä¼ å­¦å‘ç°ç¯å¢ƒå› ç´ å¯ä»¥å½±å“åŸºå› è¡¨è¾¾è€Œä¸æ”¹å˜DNAåºåˆ—ï¼Œè¿™äº›å˜åŒ–æœ‰æ—¶å¯ä»¥é—ä¼ ç»™åä»£ï¼Œä¸ºæ¼”åŒ–æä¾›äº†æ–°çš„æœºåˆ¶ã€‚å‘è‚²ç”Ÿç‰©å­¦ä¸è¿›åŒ–ç”Ÿç‰©å­¦çš„ç»“åˆäº§ç”Ÿäº†è¿›åŒ–å‘è‚²ç”Ÿç‰©å­¦ï¼ˆevo-devoï¼‰ï¼Œç ”ç©¶å‘è‚²è¿‡ç¨‹çš„æ¼”åŒ–å¦‚ä½•äº§ç”Ÿå½¢æ€å¤šæ ·æ€§ã€‚äººç±»æ¼”åŒ–æ˜¯è¿›åŒ–ç”Ÿç‰©å­¦çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ï¼Œé€šè¿‡åŒ–çŸ³è¯æ®ã€åŸºå› åˆ†æå’Œæ¯”è¾ƒè§£å‰–å­¦ç ”ç©¶äººç±»çš„èµ·æºå’Œæ¼”åŒ–å†ç¨‹ã€‚ç°ä»£äººç±»çš„éæ´²èµ·æºç†è®ºå¾—åˆ°äº†é—ä¼ å­¦è¯æ®çš„å¼ºåŠ›æ”¯æŒï¼Œæ˜¾ç¤ºæ‰€æœ‰ç°ä»£äººç±»éƒ½èµ·æºäºçº¦20ä¸‡å¹´å‰çš„éæ´²ã€‚æ–‡åŒ–æ¼”åŒ–ç ”ç©¶äººç±»æ–‡åŒ–ç‰¹å¾çš„ä¼ æ’­å’Œå˜åŒ–ï¼Œå‘ç°æ–‡åŒ–æ¼”åŒ–éµå¾ªç±»ä¼¼ç”Ÿç‰©æ¼”åŒ–çš„è§„å¾‹ã€‚ä¿æŠ¤ç”Ÿç‰©å­¦åº”ç”¨è¿›åŒ–åŸç†æ¥ä¿æŠ¤æ¿’å±ç‰©ç§å’Œç”Ÿæ€ç³»ç»Ÿï¼Œå¼ºè°ƒé—ä¼ å¤šæ ·æ€§å¯¹ç‰©ç§é•¿æœŸç”Ÿå­˜çš„é‡è¦æ€§ã€‚æ°”å€™å˜åŒ–å¯¹å½“ä»£æ¼”åŒ–äº§ç”Ÿé‡è¦å½±å“ï¼Œè®¸å¤šç‰©ç§æ­£åœ¨ç»å†å¿«é€Ÿçš„é€‚åº”æ€§æ¼”åŒ–ä»¥åº”å¯¹ç¯å¢ƒå˜åŒ–ã€‚" * 20
    },
    "128k": {
        "size": "128k",
        "context": "ç³»ç»Ÿç”Ÿç‰©å­¦æ˜¯ä¸€ä¸ªè·¨å­¦ç§‘çš„ç ”ç©¶é¢†åŸŸï¼Œæ—¨åœ¨é€šè¿‡æ•´åˆåˆ†å­ã€ç»†èƒã€ç»„ç»‡å’Œå™¨å®˜æ°´å¹³çš„ä¿¡æ¯æ¥ç†è§£ç”Ÿç‰©ç³»ç»Ÿçš„å¤æ‚æ€§å’ŒåŠŸèƒ½ã€‚è¿™ä¸ªé¢†åŸŸçš„å‡ºç°æºäºè®¤è¯†åˆ°ç”Ÿç‰©ç³»ç»Ÿçš„ç‰¹æ€§ä¸èƒ½ä»…é€šè¿‡ç ”ç©¶å…¶ç»„æˆéƒ¨åˆ†æ¥ç†è§£ï¼Œè€Œéœ€è¦è€ƒè™‘è¿™äº›éƒ¨åˆ†ä¹‹é—´çš„ç›¸äº’ä½œç”¨å’Œç³»ç»Ÿçš„æ•´ä½“è¡Œä¸ºã€‚ç³»ç»Ÿç”Ÿç‰©å­¦é‡‡ç”¨å®šé‡å’Œè®¡ç®—æ–¹æ³•æ¥å»ºæ¨¡å’Œåˆ†æç”Ÿç‰©ç½‘ç»œï¼ŒåŒ…æ‹¬åŸºå› è°ƒæ§ç½‘ç»œã€è›‹ç™½è´¨ç›¸äº’ä½œç”¨ç½‘ç»œã€ä»£è°¢ç½‘ç»œå’Œä¿¡å·ä¼ å¯¼ç½‘ç»œã€‚é«˜é€šé‡æŠ€æœ¯å¦‚åŸºå› ç»„å­¦ã€è½¬å½•ç»„å­¦ã€è›‹ç™½è´¨ç»„å­¦å’Œä»£è°¢ç»„å­¦ä¸ºç³»ç»Ÿç”Ÿç‰©å­¦æä¾›äº†å¤§é‡æ•°æ®ï¼Œä½¿ç ”ç©¶äººå‘˜èƒ½å¤ŸåŒæ—¶ç›‘æµ‹æ•°åƒä¸ªåˆ†å­çš„æ´»åŠ¨ã€‚ç”Ÿç‰©ä¿¡æ¯å­¦å’Œè®¡ç®—ç”Ÿç‰©å­¦æ˜¯ç³»ç»Ÿç”Ÿç‰©å­¦çš„æ ¸å¿ƒå·¥å…·ï¼Œç”¨äºå¤„ç†å’Œåˆ†æè¿™äº›å¤§è§„æ¨¡æ•°æ®é›†ã€‚ç½‘ç»œç”Ÿç‰©å­¦ç ”ç©¶ç”Ÿç‰©åˆ†å­ä¹‹é—´çš„ç›¸äº’ä½œç”¨ç½‘ç»œï¼Œå‘ç°äº†è®¸å¤šé‡è¦çš„ç½‘ç»œç‰¹æ€§ï¼Œå¦‚å°ä¸–ç•Œç‰¹æ€§ã€æ— æ ‡åº¦åˆ†å¸ƒå’Œæ¨¡å—åŒ–ç»“æ„ã€‚è¿™äº›ç½‘ç»œç‰¹æ€§åæ˜ äº†ç”Ÿç‰©ç³»ç»Ÿçš„é²æ£’æ€§å’Œæ•ˆç‡ã€‚åŸºå› è°ƒæ§ç½‘ç»œæ§åˆ¶åŸºå› çš„è¡¨è¾¾æ¨¡å¼ï¼Œå†³å®šç»†èƒçš„èº«ä»½å’ŒåŠŸèƒ½ã€‚è½¬å½•å› å­ã€microRNAå’Œè¡¨è§‚é—ä¼ ä¿®é¥°éƒ½å‚ä¸åŸºå› è°ƒæ§çš„å¤æ‚ç½‘ç»œã€‚ä¿¡å·ä¼ å¯¼ç½‘ç»œä½¿ç»†èƒèƒ½å¤Ÿæ„ŸçŸ¥ç¯å¢ƒå˜åŒ–å¹¶åšå‡ºé€‚å½“ååº”ï¼Œè¿™äº›ç½‘ç»œé€šå¸¸å…·æœ‰å¤šå±‚æ¬¡çš„è°ƒæ§æœºåˆ¶å’Œåé¦ˆå›è·¯ã€‚ä»£è°¢ç½‘ç»œæè¿°äº†ç»†èƒå†…åŒ–å­¦ååº”çš„ç›¸äº’å…³ç³»ï¼Œä»£è°¢æµåˆ†æå¯ä»¥é¢„æµ‹ç»†èƒåœ¨ä¸åŒæ¡ä»¶ä¸‹çš„ä»£è°¢çŠ¶æ€ã€‚ç³»ç»Ÿç”Ÿç‰©å­¦åœ¨ç–¾ç—…ç ”ç©¶ä¸­å‘æŒ¥é‡è¦ä½œç”¨ï¼Œé€šè¿‡åˆ†æç–¾ç—…ç›¸å…³çš„åˆ†å­ç½‘ç»œå˜åŒ–æ¥ç†è§£ç–¾ç—…æœºåˆ¶ã€‚ç™Œç—‡ç³»ç»Ÿç”Ÿç‰©å­¦ç ”ç©¶è‚¿ç˜¤å‘ç”Ÿå‘å±•è¿‡ç¨‹ä¸­çš„ç½‘ç»œé‡æ„ï¼Œä¸ºå¼€å‘æ–°çš„æ²»ç–—ç­–ç•¥æä¾›æŒ‡å¯¼ã€‚è¯ç‰©ç³»ç»Ÿç”Ÿç‰©å­¦ç ”ç©¶è¯ç‰©å¯¹ç”Ÿç‰©ç½‘ç»œçš„å½±å“ï¼Œæœ‰åŠ©äºè¯ç‰©å‘ç°å’Œä¸ªæ€§åŒ–åŒ»ç–—çš„å‘å±•ã€‚åˆæˆç”Ÿç‰©å­¦æ˜¯ç³»ç»Ÿç”Ÿç‰©å­¦çš„åº”ç”¨åˆ†æ”¯ï¼Œæ—¨åœ¨è®¾è®¡å’Œæ„å»ºå…·æœ‰ç‰¹å®šåŠŸèƒ½çš„ç”Ÿç‰©ç³»ç»Ÿã€‚è¿™ä¸ªé¢†åŸŸç»“åˆäº†å·¥ç¨‹å­¦åŸç†å’Œç”Ÿç‰©å­¦çŸ¥è¯†ï¼Œå¼€å‘æ ‡å‡†åŒ–çš„ç”Ÿç‰©éƒ¨ä»¶å’Œæ¨¡å—ã€‚å®šé‡ç”Ÿç‰©å­¦å¼ºè°ƒä½¿ç”¨æ•°å­¦æ¨¡å‹å’Œå®šé‡æµ‹é‡æ¥ç†è§£ç”Ÿç‰©è¿‡ç¨‹ï¼Œè¿™ç§æ–¹æ³•æœ‰åŠ©äºå‘ç°ç”Ÿç‰©ç³»ç»Ÿä¸­çš„å®šé‡è§„å¾‹å’ŒåŸç†ã€‚å•ç»†èƒæŠ€æœ¯çš„å‘å±•ä½¿ç ”ç©¶äººå‘˜èƒ½å¤Ÿåœ¨å•ä¸ªç»†èƒæ°´å¹³ä¸Šç ”ç©¶ç”Ÿç‰©ç³»ç»Ÿï¼Œæ­ç¤ºäº†ç»†èƒé—´çš„å¼‚è´¨æ€§å’ŒåŠ¨æ€å˜åŒ–ã€‚æ—¶é—´åºåˆ—åˆ†æç”¨äºç ”ç©¶ç”Ÿç‰©ç³»ç»Ÿçš„åŠ¨æ€è¡Œä¸ºï¼Œå¦‚ç»†èƒå‘¨æœŸã€æ˜¼å¤œèŠ‚å¾‹å’Œå‘è‚²è¿‡ç¨‹ã€‚å¤šå°ºåº¦å»ºæ¨¡è¯•å›¾è¿æ¥ä¸åŒç”Ÿç‰©ç»„ç»‡å±‚æ¬¡çš„æ¨¡å‹ï¼Œä»åˆ†å­åˆ°ç»†èƒå†åˆ°ç»„ç»‡å’Œå™¨å®˜ã€‚æœºå™¨å­¦ä¹ å’Œäººå·¥æ™ºèƒ½åœ¨ç³»ç»Ÿç”Ÿç‰©å­¦ä¸­çš„åº”ç”¨è¶Šæ¥è¶Šå¹¿æ³›ï¼Œç”¨äºæ¨¡å¼è¯†åˆ«ã€é¢„æµ‹å»ºæ¨¡å’Œæ•°æ®æŒ–æ˜ã€‚ä¸ªæ€§åŒ–åŒ»ç–—æ˜¯ç³»ç»Ÿç”Ÿç‰©å­¦çš„é‡è¦åº”ç”¨ç›®æ ‡ï¼Œé€šè¿‡åˆ†æä¸ªä½“çš„åˆ†å­ç‰¹å¾æ¥åˆ¶å®šä¸ªæ€§åŒ–çš„æ²»ç–—æ–¹æ¡ˆã€‚ç³»ç»Ÿå…ç–«å­¦ç ”ç©¶å…ç–«ç³»ç»Ÿçš„ç½‘ç»œç‰¹æ€§ï¼Œæœ‰åŠ©äºç†è§£å…ç–«ååº”çš„è°ƒæ§æœºåˆ¶å’Œå¼€å‘æ–°çš„å…ç–«ç–—æ³•ã€‚å†œä¸šç³»ç»Ÿç”Ÿç‰©å­¦åº”ç”¨ç³»ç»Ÿæ–¹æ³•æ¥æ”¹è‰¯ä½œç‰©ï¼Œæé«˜äº§é‡å’ŒæŠ—æ€§ã€‚ç¯å¢ƒç³»ç»Ÿç”Ÿç‰©å­¦ç ”ç©¶ç”Ÿç‰©ç³»ç»Ÿå¯¹ç¯å¢ƒå˜åŒ–çš„å“åº”ï¼Œä¸ºç¯å¢ƒä¿æŠ¤å’Œå¯æŒç»­å‘å±•æä¾›ç§‘å­¦ä¾æ®ã€‚éšç€æŠ€æœ¯çš„ä¸æ–­è¿›æ­¥å’Œæ•°æ®çš„ç§¯ç´¯ï¼Œç³»ç»Ÿç”Ÿç‰©å­¦æœ‰æœ›ä¸ºç†è§£ç”Ÿå‘½çš„å¤æ‚æ€§å’Œè§£å†³äººç±»é¢ä¸´çš„é‡å¤§æŒ‘æˆ˜åšå‡ºæ›´å¤§è´¡çŒ®ã€‚" * 24
    }
}

# æµ‹è¯•é—®é¢˜æ¨¡æ¿
TEST_QUESTIONS = [
    "è¯·æ ¹æ®ä¸Šè¿°å†…å®¹ï¼Œæ€»ç»“ä¸»è¦è§‚ç‚¹ã€‚",
    "åŸºäºæä¾›çš„ä¿¡æ¯ï¼Œåˆ†æå…¶ä¸­çš„å…³é”®æ¦‚å¿µã€‚",
    "è¯·è§£é‡Šä¸Šè¿°å†…å®¹ä¸­æœ€é‡è¦çš„ä¸‰ä¸ªè¦ç‚¹ã€‚",
    "æ ¹æ®ä¸Šä¸‹æ–‡ï¼Œè¿™ä¸ªé¢†åŸŸé¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜æ˜¯ä»€ä¹ˆï¼Ÿ",
    "è¯·ç®€è¦æ¦‚æ‹¬ä¸Šè¿°å†…å®¹çš„æ ¸å¿ƒæ€æƒ³ã€‚",
    "åŸºäºæä¾›çš„ä¿¡æ¯ï¼Œè¿™ä¸ªä¸»é¢˜çš„æœªæ¥å‘å±•è¶‹åŠ¿å¦‚ä½•ï¼Ÿ",
    "è¯·åˆ†æä¸Šè¿°å†…å®¹ä¸­æåˆ°çš„æŠ€æœ¯æˆ–æ¦‚å¿µçš„ä¼˜ç¼ºç‚¹ã€‚",
    "æ ¹æ®ä¸Šä¸‹æ–‡ï¼Œè¿™ä¸ªé¢†åŸŸå¯¹ç¤¾ä¼šçš„å½±å“æ˜¯ä»€ä¹ˆï¼Ÿ"
]

async def process_stream(stream, model_name="gpt-3.5-turbo"):
    """å¤„ç†æµå¼å“åº”å¹¶è®¡ç®—æŒ‡æ ‡"""
    first_token_time = None
    total_content = ""
    total_reasoning = ""
    chunk_count = 0
    
    try:
        async for chunk in stream:
            chunk_count += 1
            
            if first_token_time is None:
                first_token_time = time.time()
            
            # æ”¶é›†å†…å®¹
            if hasattr(chunk.choices[0].delta, 'content') and chunk.choices[0].delta.content:
                total_content += chunk.choices[0].delta.content
            
            if hasattr(chunk.choices[0].delta, "reasoning_content") and chunk.choices[0].delta.reasoning_content:
                total_reasoning += chunk.choices[0].delta.reasoning_content
            
            if chunk.choices[0].finish_reason is not None:
                break
        
        # ä½¿ç”¨æ”¹è¿›çš„tokenè®¡ç®—æ–¹æ³•
        content_tokens = calculate_tokens_accurate(total_content, model_name) if total_content else 0
        reasoning_tokens = calculate_tokens_accurate(total_reasoning, model_name) if total_reasoning else 0
            
        total_tokens = content_tokens + reasoning_tokens
        
        # å¦‚æœæ²¡æœ‰æ”¶åˆ°ä»»ä½•å†…å®¹ï¼Œä½†æ”¶åˆ°äº† chunkï¼Œè¯´æ˜å¯èƒ½æœ‰é—®é¢˜
        if chunk_count > 0 and total_tokens == 0:
            # è‡³å°‘è¿”å› 1 ä¸ª tokenï¼Œè¡¨ç¤ºæ”¶åˆ°äº†å“åº”
            total_tokens = 1
            content_tokens = 1
        
        # è¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯
        if total_content:
            chinese_chars = len([c for c in total_content if '\u4e00' <= c <= '\u9fff'])
            english_chars = len(total_content) - chinese_chars
            logging.debug(f"Content analysis: total_len={len(total_content)}, chinese_chars={chinese_chars}, english_chars={english_chars}")
        
        logging.debug(f"Stream processed: {chunk_count} chunks, content_len={len(total_content)}, reasoning_len={len(total_reasoning)}, content_tokens={content_tokens}, reasoning_tokens={reasoning_tokens}, total_tokens={total_tokens}")
        logging.debug(f"Content preview: {total_content[:100]}{'...' if len(total_content) > 100 else ''}")
        
        return first_token_time, total_tokens, content_tokens, reasoning_tokens
        
    except Exception as e:
        logging.error(f"Error processing stream: {e}")
        # å¦‚æœå¤„ç†æµæ—¶å‡ºé”™ï¼Œä½†å·²ç»æ”¶åˆ°äº†ç¬¬ä¸€ä¸ª tokenï¼Œä»ç„¶è¿”å›éƒ¨åˆ†ç»“æœ
        if first_token_time is not None:
            estimated_tokens = calculate_tokens_accurate(total_content, model_name) if total_content else 1
            return first_token_time, estimated_tokens, estimated_tokens, 0
        else:
            raise e

async def make_context_request(client, model, context_size, output_tokens, request_timeout, request_id=None):
    """å‘é€å¸¦æœ‰æŒ‡å®šä¸Šä¸‹æ–‡å¤§å°çš„è¯·æ±‚"""
    start_time = time.time()
    
    # è·å–å¯¹åº”å¤§å°çš„ä¸Šä¸‹æ–‡
    context_template = CONTEXT_TEMPLATES[context_size]
    context_content = context_template["context"]
    
    # å¯¹äº 13tï¼Œä¸æ‹¼æ¥é¢å¤–çš„é—®é¢˜
    if context_size == "13t":
        full_prompt = context_content
        question = "å†…ç½®é—®é¢˜ï¼ˆ13tæµ‹è¯•ï¼‰"
    else:
        question = random.choice(TEST_QUESTIONS)
        # ç»„åˆå®Œæ•´çš„æç¤º
        full_prompt = f"{context_content}\n\n{question}"
    
    # è®¡ç®—ä¸Šä¸‹æ–‡å¤§å°ï¼ˆå­—ç¬¦æ•°å’Œtokenä¼°ç®—ï¼‰
    context_char_count = len(context_content)
    prompt_char_count = len(full_prompt)
    # ä½¿ç”¨æ”¹è¿›çš„tokenè®¡ç®—æ–¹æ³•
    prompt_tokens_estimate = calculate_tokens_accurate(full_prompt, model)
    
    try:
        logging.debug(f"Request {request_id}: Sending request with prompt length {len(full_prompt)}")
        
        stream = await client.chat.completions.create(
            model=model,
            messages=[
                {"role": "user", "content": full_prompt}
            ],
            max_tokens=output_tokens,
            stream=True,
            # æ·»åŠ  SSE ç›¸å…³å‚æ•°
            stream_options={"include_usage": True} if hasattr(client, 'stream_options') else None
        )
        
        logging.debug(f"Request {request_id}: Stream created, processing...")
        
        first_token_time, total_tokens, content_tokens, reasoning_tokens = await asyncio.wait_for(
            process_stream(stream, model), timeout=request_timeout
        )
        
        logging.debug(f"Request {request_id}: Stream processed successfully, tokens={total_tokens}")
        
        end_time = time.time()
        elapsed_time = end_time - start_time
        ttft = first_token_time - start_time if first_token_time else None
        
        # è®¡ç®—ä¸åŒçš„ååé‡æŒ‡æ ‡
        generation_throughput = total_tokens / elapsed_time if elapsed_time > 0 and total_tokens > 0 else 0
        prompt_throughput = prompt_tokens_estimate / ttft if ttft and ttft > 0 else 0
        
        return {
            "success": True,
            "request_id": request_id,
            "context_size": context_size,
            "context_char_count": context_char_count,
            "prompt_char_count": prompt_char_count,
            "prompt_tokens_estimate": prompt_tokens_estimate,
            "total_tokens": total_tokens,
            "content_tokens": content_tokens,
            "reasoning_tokens": reasoning_tokens,
            "elapsed_time": elapsed_time,
            "generation_throughput": generation_throughput,
            "prompt_throughput": prompt_throughput,
            "ttft": ttft,
            "question": question,
            "start_time": start_time,
            "end_time": end_time
        }
        
    except asyncio.TimeoutError:
        logging.warning(f"Request {request_id} with context size {context_size} timed out after {request_timeout} seconds")
        return {
            "success": False,
            "request_id": request_id,
            "context_size": context_size,
            "error": "timeout",
            "context_char_count": context_char_count,
            "prompt_char_count": prompt_char_count,
            "prompt_tokens_estimate": prompt_tokens_estimate
        }
    except Exception as e:
        logging.error(f"Error during request {request_id} with context size {context_size}: {str(e)}")
        logging.error(f"Request details - prompt_length: {len(full_prompt)}, model: {model}")
        return {
            "success": False,
            "request_id": request_id,
            "context_size": context_size,
            "error": str(e),
            "error_type": type(e).__name__,
            "context_char_count": context_char_count,
            "prompt_char_count": prompt_char_count,
            "prompt_tokens_estimate": prompt_tokens_estimate
        }

async def test_sse_connection(llm_url, api_key, model):
    """æµ‹è¯• SSE è¿æ¥æ˜¯å¦æ­£å¸¸å·¥ä½œ"""
    client = AsyncOpenAI(base_url=llm_url, api_key=api_key)
    
    try:
        print("\nğŸ” æµ‹è¯• SSE è¿æ¥...")
        
        # ç®€å•çš„æµ‹è¯•è¯·æ±‚
        test_prompt = "è¯·è¯´'ä½ å¥½'"
        
        stream = await client.chat.completions.create(
            model=model,
            messages=[{"role": "user", "content": test_prompt}],
            max_tokens=10,
            stream=True
        )
        
        chunk_count = 0
        content_received = ""
        
        async for chunk in stream:
            chunk_count += 1
            if hasattr(chunk.choices[0].delta, 'content') and chunk.choices[0].delta.content:
                content_received += chunk.choices[0].delta.content
            
            if chunk.choices[0].finish_reason is not None:
                break
        
        if chunk_count > 0:
            print(f"âœ… SSE è¿æ¥æ­£å¸¸ - æ”¶åˆ° {chunk_count} ä¸ªæ•°æ®å—ï¼Œå†…å®¹é•¿åº¦: {len(content_received)}")
            if content_received:
                print(f"   æ”¶åˆ°å†…å®¹: {content_received[:50]}{'...' if len(content_received) > 50 else ''}")
            return True
        else:
            print("âŒ SSE è¿æ¥å¼‚å¸¸ - æœªæ”¶åˆ°ä»»ä½•æ•°æ®å—")
            return False
            
    except Exception as e:
        print(f"âŒ SSE è¿æ¥æµ‹è¯•å¤±è´¥: {e}")
        return False

async def run_context_benchmark(context_sizes, num_requests_per_size, output_tokens, llm_url, api_key, model, request_timeout, concurrency=1):
    """è¿è¡Œä¸Šä¸‹æ–‡åŸºå‡†æµ‹è¯•"""
    client = AsyncOpenAI(base_url=llm_url, api_key=api_key)
    all_results = []
    
    for context_size in context_sizes:
        print(f"\næµ‹è¯•ä¸Šä¸‹æ–‡å¤§å°: {context_size} ({CONTEXT_TEMPLATES[context_size]['size']})")
        print(f"ä¸Šä¸‹æ–‡å­—ç¬¦æ•°: {len(CONTEXT_TEMPLATES[context_size]['context']):,}")
        print(f"å¹¶å‘æ•°: {concurrency}")
        
        size_results = []
        test_failed = False  # æ ‡è®°å½“å‰æµ‹è¯•æ˜¯å¦å¤±è´¥
        
        # å¦‚æœå¹¶å‘æ•°ä¸º1ï¼ŒæŒ‰é¡ºåºæ‰§è¡Œ
        if concurrency == 1:
            for i in range(num_requests_per_size):
                print(f"  è¯·æ±‚ {i+1}/{num_requests_per_size}...", end=" ")
                result = await make_context_request(client, model, context_size, output_tokens, request_timeout, f"{context_size}-{i+1}")
                size_results.append(result)
                
                if result["success"]:
                    print(f"æˆåŠŸ - å»¶è¿Ÿ: {result['elapsed_time']:.2f}s, ç”ŸæˆTPS: {result['generation_throughput']:.1f}, TTFT: {result['ttft']:.3f}s")
                else:
                    print(f"å¤±è´¥ - {result.get('error', 'unknown error')}")
                    print(f"  æ£€æµ‹åˆ°è¯·æ±‚å¤±è´¥ï¼Œåœæ­¢å½“å‰ä¸Šä¸‹æ–‡å¤§å°çš„åç»­è¯·æ±‚")
                    test_failed = True
                    break  # å¤±è´¥æ—¶åœæ­¢å‘é€åç»­è¯·æ±‚
                
                # è¯·æ±‚é—´éš”ï¼Œé¿å…è¿‡è½½
                await asyncio.sleep(1)
        else:
            # å¹¶å‘æ‰§è¡Œ
            print(f"  å¹¶å‘æ‰§è¡Œ {num_requests_per_size} ä¸ªè¯·æ±‚...")
            
            # åˆ›å»ºå¹¶å‘ä»»åŠ¡
            tasks = []
            for i in range(num_requests_per_size):
                task = make_context_request(client, model, context_size, output_tokens, request_timeout, f"{context_size}-{i+1}")
                tasks.append(task)
            
            # æ§åˆ¶å¹¶å‘æ•°
            semaphore = asyncio.Semaphore(concurrency)
            
            async def limited_request(task):
                async with semaphore:
                    return await task
            
            # æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
            batch_start_time = time.time()
            results = await asyncio.gather(*[limited_request(task) for task in tasks], return_exceptions=True)
            batch_end_time = time.time()
            
            # å¤„ç†ç»“æœå¹¶æ£€æŸ¥å¤±è´¥
            has_failure = False
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    result = {
                        "success": False,
                        "request_id": f"{context_size}-{i+1}",
                        "context_size": context_size,
                        "error": str(result),
                        "context_char_count": len(CONTEXT_TEMPLATES[context_size]['context']),
                        "prompt_char_count": 0,
                        "prompt_tokens_estimate": 0
                    }
                    has_failure = True
                elif not result["success"]:
                    has_failure = True
                size_results.append(result)
            
            # å¦‚æœæœ‰å¤±è´¥ï¼Œæç¤ºç”¨æˆ·
            if has_failure:
                failed_count = len([r for r in size_results if not r["success"]])
                print(f"  æ£€æµ‹åˆ° {failed_count} ä¸ªè¯·æ±‚å¤±è´¥ï¼Œå½“å‰ä¸Šä¸‹æ–‡å¤§å°æµ‹è¯•å®Œæˆ")
                test_failed = True
            
            # æ˜¾ç¤ºæ‰¹æ¬¡ç»“æœ
            successful_requests = [r for r in size_results if r["success"]]
            batch_duration = batch_end_time - batch_start_time
            
            print(f"  æ‰¹æ¬¡å®Œæˆ - æ€»æ—¶é—´: {batch_duration:.2f}s, æˆåŠŸ: {len(successful_requests)}/{len(size_results)}")
            if successful_requests:
                avg_ttft = np.mean([r['ttft'] for r in successful_requests if r['ttft'] is not None])
                avg_gen_tps = np.mean([r['generation_throughput'] for r in successful_requests])
                avg_prompt_tps = np.mean([r['prompt_throughput'] for r in successful_requests if r['prompt_throughput'] > 0])
                print(f"  å¹³å‡æŒ‡æ ‡ - TTFT: {avg_ttft:.3f}s, ç”ŸæˆTPS: {avg_gen_tps:.1f}, æç¤ºTPS: {avg_prompt_tps:.1f}")
        
        all_results.append({
            "context_size": context_size,
            "concurrency": concurrency,
            "results": size_results
        })
        
        # å¦‚æœå½“å‰æµ‹è¯•å¤±è´¥ï¼Œåœæ­¢æµ‹è¯•åç»­çš„ä¸Šä¸‹æ–‡å¤§å°
        if test_failed:
            print(f"\næ£€æµ‹åˆ°æµ‹è¯•å¤±è´¥ï¼Œåœæ­¢æµ‹è¯•åç»­çš„ä¸Šä¸‹æ–‡å¤§å°")
            print(f"å·²å®Œæˆçš„æµ‹è¯•: {[result['context_size'] for result in all_results]}")
            break
    
    return all_results

def analyze_context_results(all_results):
    """åˆ†æä¸Šä¸‹æ–‡æµ‹è¯•ç»“æœ"""
    summary = []
    
    for size_group in all_results:
        context_size = size_group["context_size"]
        concurrency = size_group.get("concurrency", 1)
        results = size_group["results"]
        
        # è¿‡æ»¤æˆåŠŸçš„è¯·æ±‚
        successful_results = [r for r in results if r["success"]]
        
        if not successful_results:
            summary.append({
                "context_size": context_size,
                "concurrency": concurrency,
                "success_rate": 0,
                "avg_latency": None,
                "avg_generation_tps": None,
                "avg_prompt_tps": None,
                "avg_ttft": None,
                "min_ttft": None,
                "max_ttft": None,
                "context_chars": results[0]["context_char_count"] if results else 0,
                "prompt_chars": results[0]["prompt_char_count"] if results else 0,
                "prompt_tokens": results[0]["prompt_tokens_estimate"] if results else 0,
                "total_requests": len(results),
                "successful_requests": 0
            })
            continue
        
        # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
        success_rate = len(successful_results) / len(results) * 100
        latencies = [r["elapsed_time"] for r in successful_results]
        generation_tps_values = [r["generation_throughput"] for r in successful_results]
        prompt_tps_values = [r["prompt_throughput"] for r in successful_results if r["prompt_throughput"] > 0]
        ttft_values = [r["ttft"] for r in successful_results if r["ttft"] is not None]
        
        avg_latency = np.mean(latencies) if latencies else None
        avg_generation_tps = np.mean(generation_tps_values) if generation_tps_values else None
        avg_prompt_tps = np.mean(prompt_tps_values) if prompt_tps_values else None
        avg_ttft = np.mean(ttft_values) if ttft_values else None
        
        p95_latency = np.percentile(latencies, 95) if latencies else None
        p95_generation_tps = np.percentile(generation_tps_values, 95) if generation_tps_values else None
        p95_prompt_tps = np.percentile(prompt_tps_values, 95) if prompt_tps_values else None
        p95_ttft = np.percentile(ttft_values, 95) if ttft_values else None
        
        # è®¡ç®—æœ€å°å’Œæœ€å¤§TTFT
        min_ttft = np.min(ttft_values) if ttft_values else None
        max_ttft = np.max(ttft_values) if ttft_values else None
        
        summary.append({
            "context_size": context_size,
            "concurrency": concurrency,
            "success_rate": success_rate,
            "avg_latency": avg_latency,
            "p95_latency": p95_latency,
            "avg_generation_tps": avg_generation_tps,
            "p95_generation_tps": p95_generation_tps,
            "avg_prompt_tps": avg_prompt_tps,
            "p95_prompt_tps": p95_prompt_tps,
            "avg_ttft": avg_ttft,
            "p95_ttft": p95_ttft,
            "min_ttft": min_ttft,
            "max_ttft": max_ttft,
            "context_chars": successful_results[0]["context_char_count"],
            "prompt_chars": successful_results[0]["prompt_char_count"],
            "prompt_tokens": successful_results[0]["prompt_tokens_estimate"],
            "total_requests": len(results),
            "successful_requests": len(successful_results)
        })
    
    return summary

def print_context_summary(summary, model_name):
    """æ‰“å°ä¸Šä¸‹æ–‡æµ‹è¯•ç»“æœæ±‡æ€»"""
    console = Console(width=120)
    
    # åˆ›å»ºæ ‡é¢˜é¢æ¿
    title = Text("LLM ä¸Šä¸‹æ–‡å¤§å°æ€§èƒ½æµ‹è¯•æŠ¥å‘Š", style="bold")
    console.print(Panel(title, width=80))
    
    # æ‰“å°åŸºæœ¬ä¿¡æ¯
    basic_info = Table(show_header=False, width=60)
    basic_info.add_column("é¡¹ç›®", style="cyan", width=20)
    basic_info.add_column("å€¼", style="green", width=40)
    
    basic_info.add_row("æµ‹è¯•æ¨¡å‹", model_name)
    basic_info.add_row("æµ‹è¯•ç±»å‹", "ä¸Šä¸‹æ–‡å¤§å°æ€§èƒ½æµ‹è¯•")
    basic_info.add_row("æµ‹è¯•æ—¶é—´", time.strftime("%Y-%m-%d %H:%M:%S"))
    
    console.print("\nåŸºæœ¬ä¿¡æ¯:")
    console.print(basic_info)
    
    # åˆ›å»ºè¯¦ç»†ç»“æœè¡¨æ ¼
    table = Table(
        title="ä¸Šä¸‹æ–‡å¤§å°æ€§èƒ½å¯¹æ¯”",
        show_header=True,
        header_style="bold cyan",
        border_style="blue",
        width=120
    )
    
    # æ·»åŠ åˆ—
    table.add_column("ä¸Šä¸‹æ–‡å¤§å°", justify="center", style="cyan", width=8)
    table.add_column("å¹¶å‘æ•°", justify="center", width=6)
    table.add_column("å­—ç¬¦æ•°", justify="right", width=8)
    table.add_column("æˆåŠŸç‡", justify="right", width=6)
    table.add_column("å¹³å‡å»¶è¿Ÿ(s)", justify="right", width=10)
    table.add_column("ç”ŸæˆTPS", justify="right", width=8)
    table.add_column("æç¤ºTPS", justify="right", width=8)
    table.add_column("æœ€å°TTFT(s)", justify="right", width=10)
    table.add_column("æœ€å¤§TTFT(s)", justify="right", width=10)
    table.add_column("å¹³å‡TTFT(s)", justify="right", width=10)
    
    # æ·»åŠ æ•°æ®è¡Œ
    for row in summary:
        # æ ¹æ®æˆåŠŸç‡è®¾ç½®è¡Œæ ·å¼
        success_rate = row["success_rate"]
        row_style = "green" if success_rate >= 95 else "yellow" if success_rate >= 80 else "red"
        
        table.add_row(
            row["context_size"],
            str(row["concurrency"]),
            f"{row['context_chars']:,}",
            f"{success_rate:.1f}%",
            f"{row['avg_latency']:.3f}" if row['avg_latency'] is not None else "N/A",
            f"{row['avg_generation_tps']:.1f}" if row['avg_generation_tps'] is not None else "N/A",
            f"{row['avg_prompt_tps']:.1f}" if row['avg_prompt_tps'] is not None else "N/A",
            f"{row['min_ttft']:.3f}" if row['min_ttft'] is not None else "N/A",
            f"{row['max_ttft']:.3f}" if row['max_ttft'] is not None else "N/A",
            f"{row['avg_ttft']:.3f}" if row['avg_ttft'] is not None else "N/A",
            style=row_style
        )
    
    console.print("\n")
    console.print(table)
    
    # æ€§èƒ½åˆ†æ
    valid_results = [r for r in summary if r["avg_latency"] is not None]
    if valid_results:
        console.print("\næ€§èƒ½åˆ†æ:", style="bold cyan")
        
        # æ‰¾å‡ºæœ€ä½³å’Œæœ€å·®æ€§èƒ½
        best_latency = min(valid_results, key=lambda x: x["avg_latency"])
        worst_latency = max(valid_results, key=lambda x: x["avg_latency"])
        best_generation_tps = max(valid_results, key=lambda x: x["avg_generation_tps"] or 0)
        best_ttft = min([r for r in valid_results if r["avg_ttft"] is not None], key=lambda x: x["avg_ttft"], default=None)
        
        console.print(f"â€¢ æœ€ä½å»¶è¿Ÿ: {best_latency['context_size']} ({best_latency['avg_latency']:.3f}s)", style="green")
        console.print(f"â€¢ æœ€é«˜å»¶è¿Ÿ: {worst_latency['context_size']} ({worst_latency['avg_latency']:.3f}s)", style="red")
        console.print(f"â€¢ æœ€é«˜ç”ŸæˆTPS: {best_generation_tps['context_size']} ({best_generation_tps['avg_generation_tps']:.1f} tokens/s)", style="green")
        if best_ttft:
            console.print(f"â€¢ æœ€ä½³TTFT: {best_ttft['context_size']} ({best_ttft['avg_ttft']:.3f}s)", style="green")
        
        # å»¶è¿Ÿå¢é•¿åˆ†æ
        if len(valid_results) > 1:
            latency_increase = (worst_latency["avg_latency"] - best_latency["avg_latency"]) / best_latency["avg_latency"] * 100
            console.print(f"â€¢ å»¶è¿Ÿå¢é•¿: {latency_increase:.1f}% (ä»æœ€å°åˆ°æœ€å¤§ä¸Šä¸‹æ–‡)", style="yellow")

def main():
    parser = argparse.ArgumentParser(description="æµ‹è¯• LLM æ¨¡å‹åœ¨ä¸åŒä¸Šä¸‹æ–‡å¤§å°ä¸‹çš„æ€§èƒ½")
    parser.add_argument("--llm_url", type=str, required=True, help="LLM æœåŠ¡å™¨ URL")
    parser.add_argument("--api_key", type=str, required=False, default="default", help="API å¯†é’¥")
    parser.add_argument("--model", type=str, default="deepseek-r1", help="æ¨¡å‹åç§° (é»˜è®¤: deepseek-r1)")
    parser.add_argument("--context_sizes", type=str, default="13t,1k,2k,4k,8k,16k,32k,64k,92k,128k", 
                       help="è¦æµ‹è¯•çš„ä¸Šä¸‹æ–‡å¤§å°ï¼Œç”¨é€—å·åˆ†éš” (é»˜è®¤: 13t,1k,2k,4k,8k,16k,32k,64k,92k,128k)")
    parser.add_argument("--num_requests", type=int, default=3, 
                       help="æ¯ä¸ªä¸Šä¸‹æ–‡å¤§å°çš„è¯·æ±‚æ¬¡æ•° (é»˜è®¤: 3)")
    parser.add_argument("--output_tokens", type=int, default=200, 
                       help="è¾“å‡º token æ•°é‡ (é»˜è®¤: 200)")
    parser.add_argument("--request_timeout", type=int, default=120, 
                       help="è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ (é»˜è®¤: 120)")
    parser.add_argument("--concurrency", type=int, default=1, 
                       help="å¹¶å‘è¯·æ±‚æ•° (é»˜è®¤: 1)")
    parser.add_argument("--debug", action="store_true", 
                       help="å¯ç”¨è°ƒè¯•æ¨¡å¼ï¼Œæ˜¾ç¤ºè¯¦ç»†æ—¥å¿—")
    parser.add_argument("--skip_sse_test", action="store_true", 
                       help="è·³è¿‡ SSE è¿æ¥æµ‹è¯•")
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—çº§åˆ«
    if args.debug:
        logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')
    else:
        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    
    # è§£æä¸Šä¸‹æ–‡å¤§å°
    context_sizes = [size.strip() for size in args.context_sizes.split(",")]
    
    # éªŒè¯ä¸Šä¸‹æ–‡å¤§å°
    valid_sizes = list(CONTEXT_TEMPLATES.keys())
    for size in context_sizes:
        if size not in valid_sizes:
            print(f"é”™è¯¯: æ— æ•ˆçš„ä¸Šä¸‹æ–‡å¤§å° '{size}'. å¯ç”¨é€‰é¡¹: {', '.join(valid_sizes)}")
            return
    
    print(f"å¼€å§‹ä¸Šä¸‹æ–‡æ€§èƒ½æµ‹è¯•...")
    print(f"æ¨¡å‹: {args.model}")
    print(f"æµ‹è¯•çš„ä¸Šä¸‹æ–‡å¤§å°: {', '.join(context_sizes)}")
    print(f"æ¯ä¸ªå¤§å°çš„è¯·æ±‚æ¬¡æ•°: {args.num_requests}")
    print(f"è¾“å‡º token æ•°: {args.output_tokens}")
    print(f"å¹¶å‘æ•°: {args.concurrency}")
    print(f"è¯·æ±‚è¶…æ—¶: {args.request_timeout}ç§’")
    
    # SSE è¿æ¥æµ‹è¯•
    if not args.skip_sse_test:
        sse_success = asyncio.run(test_sse_connection(args.llm_url, args.api_key, args.model))
        if not sse_success:
            print("\nâš ï¸  SSE è¿æ¥æµ‹è¯•å¤±è´¥ï¼Œä½†ç»§ç»­è¿›è¡ŒåŸºå‡†æµ‹è¯•...")
            print("   å¦‚æœæµ‹è¯•æŒç»­å¤±è´¥ï¼Œè¯·æ£€æŸ¥æœåŠ¡å™¨é…ç½®å’Œç½‘ç»œè¿æ¥")
        print()
    
    # è¿è¡Œæµ‹è¯•
    all_results = asyncio.run(run_context_benchmark(
        context_sizes,
        args.num_requests,
        args.output_tokens,
        args.llm_url,
        args.api_key,
        args.model,
        args.request_timeout,
        args.concurrency
    ))
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    import os
    output_dir = "outputs"
    os.makedirs(output_dir, exist_ok=True)
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    timestamp = time.strftime("%Y%m%d_%H%M%S")
    results_file = os.path.join(output_dir, f"context_benchmark_results_{timestamp}.json")
    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump(all_results, f, indent=2, ensure_ascii=False)
    print(f"\nè¯¦ç»†æµ‹è¯•ç»“æœå·²ä¿å­˜è‡³: {results_file}")
    
    # åˆ†æå’Œæ˜¾ç¤ºç»“æœ
    summary = analyze_context_results(all_results)
    print_context_summary(summary, args.model)
    
    # ä¿å­˜æ±‡æ€»ç»“æœ
    summary_file = os.path.join(output_dir, f"context_benchmark_summary_{timestamp}.json")
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(summary, f, indent=2, ensure_ascii=False)
    print(f"\næ±‡æ€»ç»“æœå·²ä¿å­˜è‡³: {summary_file}")

if __name__ == "__main__":
    main()